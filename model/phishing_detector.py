#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
phishing_detector.py â€“ Train multiple models for phishing detection
===================================================================
Reads feature file generated by feature_extractor.py, balances data with SMOTE,
trains multiple classifiers (LogReg, RF, XGB, LGBM, GBC), and saves best model.

Outputs:
    - outputs/<BestModel>_balanced.pkl
    - outputs/scaler.pkl
    - outputs/model_report.json
    - outputs/feature_columns.json
    - outputs/feature_importance.json
"""

import os
import json
import joblib
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from imblearn.over_sampling import SMOTE, RandomOverSampler
from collections import Counter

# -----------------------------
# Configuration
# -----------------------------
DATA_PATH = "features_dataset.csv"  # file created by feature_extractor
os.makedirs("outputs", exist_ok=True)
RANDOM_STATE = 42

# -----------------------------
# Load dataset
# -----------------------------
print(f"ğŸ“¥ Loading dataset: {DATA_PATH}")
df = pd.read_csv(DATA_PATH).fillna(0)

if "label" not in df.columns:
    raise ValueError("âŒ Dataset must contain a 'label' column (1=phishing, 0=benign).")

# Drop non-feature columns
drop_cols = [c for c in ["source", "Source", "domain", "url", "Domain", "URL"] if c in df.columns]
df = df.drop(columns=drop_cols, errors="ignore")

# Encode non-numeric columns
for col in df.columns:
    if df[col].dtype == "object":
        print(f"âš™ï¸ Encoding text column: {col}")
        df[col] = df[col].astype("category").cat.codes

# Split features/labels
X = df.drop(columns=["label"])
y = df["label"]

print(f"ğŸ“Š Original data shape: {X.shape}")
print(f"ğŸ“Š Class distribution: {dict(Counter(y))}")

# Save feature columns for inference consistency
feature_columns = list(X.columns)
json.dump(feature_columns, open("outputs/feature_columns.json", "w"), indent=2)

# -----------------------------
# Train-Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y
)

# -----------------------------
# Handle imbalance
# -----------------------------
print("âš–ï¸  Applying SMOTE / oversampling to balance classes...")
try:
    smote = SMOTE(random_state=RANDOM_STATE)
    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
    print(f"âœ… After SMOTE: {dict(Counter(y_train_bal))}")
except Exception as e:
    print(f"âš ï¸ SMOTE failed ({e}) â€” using RandomOverSampler instead.")
    ros = RandomOverSampler(random_state=RANDOM_STATE)
    X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)
    print(f"âœ… After Random Oversampling: {dict(Counter(y_train_bal))}")

# -----------------------------
# Scale features
# -----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_bal)
X_test_scaled = scaler.transform(X_test)

# -----------------------------
# Define models
# -----------------------------
models = {
    "LogisticRegression": LogisticRegression(max_iter=300, n_jobs=-1, class_weight="balanced"),
    "RandomForest": RandomForestClassifier(
        n_estimators=300, max_depth=12, random_state=RANDOM_STATE,
        n_jobs=-1, class_weight="balanced"
    ),
    "XGBoost": XGBClassifier(
        n_estimators=300, learning_rate=0.05, max_depth=10,
        subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE,
        n_jobs=-1, scale_pos_weight=(len(y_train_bal[y_train_bal==0]) / max(1,len(y_train_bal[y_train_bal==1]))),
        eval_metric="logloss"
    ),
    "LightGBM": LGBMClassifier(
        n_estimators=300, learning_rate=0.05, num_leaves=64,
        colsample_bytree=0.9, subsample=0.8, random_state=RANDOM_STATE,
        class_weight="balanced", n_jobs=-1
    ),
    "GradientBoosting": GradientBoostingClassifier(
        n_estimators=200, learning_rate=0.08, max_depth=6, random_state=RANDOM_STATE
    ),
}

# -----------------------------
# Train and evaluate
# -----------------------------
report = {}
best_model = None
best_score = 0
feature_importances = {}

for name, model in models.items():
    print(f"\nğŸš€ Training {name}...")

    if name == "LogisticRegression":
        model.fit(X_train_scaled, y_train_bal)
        preds = model.predict(X_test_scaled)
        probs = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train_bal, y_train_bal)
        preds = model.predict(X_test)
        probs = model.predict_proba(X_test)[:, 1]

    auc = roc_auc_score(y_test, probs)
    cr = classification_report(y_test, preds, output_dict=True, zero_division=0)
    f1 = cr["weighted avg"]["f1-score"]
    prec = cr["weighted avg"]["precision"]
    rec = cr["weighted avg"]["recall"]

    print(f"âœ… {name}: F1={f1:.4f} | Precision={prec:.4f} | Recall={rec:.4f} | AUC={auc:.4f}")
    print(confusion_matrix(y_test, preds))

    report[name] = {
        "F1": round(f1, 4),
        "Precision": round(prec, 4),
        "Recall": round(rec, 4),
        "AUC": round(auc, 4)
    }

    # Save feature importances if available
    if hasattr(model, "feature_importances_"):
        feature_importances[name] = dict(zip(feature_columns, model.feature_importances_.tolist()))

    # Track best model by F1-score
    if f1 > best_score:
        best_score = f1
        best_model = (name, model)

# -----------------------------
# Save best model
# -----------------------------
if best_model is None:
    raise RuntimeError("âŒ No model trained successfully.")

best_name, best_clf = best_model
joblib.dump(best_clf, f"outputs/{best_name}_balanced.pkl")
joblib.dump(scaler, "outputs/scaler.pkl")

with open("outputs/model_report.json", "w") as f:
    json.dump(report, f, indent=2)

if feature_importances:
    with open("outputs/feature_importance.json", "w") as f:
        json.dump(feature_importances, f, indent=2)

print(f"\nğŸ† Best Model: {best_name}")
print(f"ğŸ“ˆ Best F1 = {best_score:.4f}")
print("ğŸ“ All models and reports saved in /outputs/")

# -----------------------------
# Diagnostics
# -----------------------------
print("\nğŸ” Model Summary:")
print(pd.DataFrame(report).T.sort_values(by="F1", ascending=False))

# Basic warning if model might be degenerate
if best_score < 0.6 or all(v["Recall"] < 0.5 for v in report.values()):
    print("âš ï¸ Warning: Low recall or overfitting detected. Check dataset balance and feature consistency.")
